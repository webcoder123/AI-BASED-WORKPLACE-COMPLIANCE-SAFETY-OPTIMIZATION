# -*- coding: utf-8 -*-
"""Complete__all_pose and object.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hHe2A-A0QqwSKuhO6w6dkvEFNDsbAjuR
"""

pip install ultralytics

import cv2
import numpy as np
from ultralytics import YOLO

# Load models
pose_model = YOLO("/content/yolo11n-pose.pt")  # Pose estimation model
object_model = YOLO("/content/best.pt")  # Object detection model

# Function to calculate angle between three points
def calculate_angle(a, b, c):
    if a is None or b is None or c is None:
        return None
    a, b, c = np.array(a), np.array(b), np.array(c)
    ba = a - b
    bc = c - b
    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)
    angle = np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))
    return angle

# Define action classification based on keypoints
def classify_pose(keypoints):
    if keypoints is None or len(keypoints) < 9:
        return "Unknown"

    nose, left_shoulder, right_shoulder, left_hip, right_hip, left_knee, right_knee, left_ankle, right_ankle = keypoints[:9]
    keypoint_check = all(kp is not None for kp in [left_hip, left_knee, left_ankle, right_hip, right_knee, right_ankle])
    if not keypoint_check:
        return "Unknown"

    left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)
    right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)
    torso_angle = calculate_angle(left_shoulder, left_hip, left_knee)
    arm_angle = calculate_angle(left_shoulder, nose, right_shoulder)

    if None in [left_knee_angle, right_knee_angle, torso_angle, arm_angle]:
        return "Unknown"

    stride_length = abs(left_ankle[0] - right_ankle[0])
    knee_difference = abs(left_knee[1] - right_knee[1])
    left_wrist = keypoints[10] if len(keypoints) > 10 else None
    right_wrist = keypoints[11] if len(keypoints) > 11 else None

    if torso_angle < 45:
        return "Bending"
    elif left_knee_angle < 100 or right_knee_angle < 100:
        if stride_length > 50 and knee_difference > 30:
            return "Running"
        return "Walking"
    elif left_knee_angle > 160 and right_knee_angle > 160:
        return "Standing"
    elif nose[1] > left_hip[1] and nose[1] > right_hip[1]:
        return "Lying on Floor"
    elif arm_angle > 120:
        return "Arm Raising"
    elif left_hip[1] < left_knee[1] and right_hip[1] < right_knee[1]:
        return "Jumping"
    elif left_shoulder[0] != right_shoulder[0]:
        return "Leaning"
    elif left_ankle[1] < left_knee[1] and right_ankle[1] < right_knee[1]:
        return "Climbing"
    elif left_wrist and right_wrist and (left_wrist[0] - left_hip[0] < 30 or right_wrist[0] - right_hip[0] < 30):
        return "Touching"
    return "Person"

# Video input
input_video = "/content/Vedio_01.mp4"
cap = cv2.VideoCapture(input_video)
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
fps = int(cap.get(cv2.CAP_PROP_FPS))

output_path = "merged_output_1.avi"
fourcc = cv2.VideoWriter_fourcc(*"XVID")
video_writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# Process video
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Object detection
    object_results = object_model.predict(source=frame, conf=0.5)

    # Pose estimation
    pose_results = pose_model.predict(source=frame, conf=0.5)

   # Draw object detection results
    for result in object_results:
      for box in result.boxes:  # Iterate through boxes within the result
          x1, y1, x2, y2 = map(int, box.xyxy[0])
          label = object_model.names[int(box.cls[0])]  # Access cls from the box
          cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)
          cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

    # Draw pose estimation results
    for result in pose_results:
        for box, keypoints in zip(result.boxes, result.keypoints.xy):
            keypoints = [tuple(map(int, kp)) for kp in keypoints]
            action = classify_pose(keypoints)

            x1, y1, x2, y2 = map(int, box.xyxy[0])
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f"{action}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    video_writer.write(frame)

video_writer.release()
cap.release()
print(f"Processed video saved at: {output_path}")